from dataclasses import dataclass
import re
import subprocess
import concurrent.futures
import os

@dataclass
class URL:
    url: str
    entry_id: str
    domain: str


def download_single(url: URL):
    """Downloads the file from a single url."""
    output_path = rf"G:\lecture-backup\{url.entry_id}.mp4"
    if os.path.exists(output_path): # should not redownload downloaded videos
        print(f"{url.entry_id} already downloaded, continuing")
        return
    print(f"Downloading {url.entry_id}")
    command = [
        "yt-dlp",
        "--output", output_path, 
        url.url
    ]
    try:
        subprocess.run(command)
    except Exception as e:
        print(f"Error with video {url.entry_id}: {e}")


def download_all(urls: list[URL]):
    """Downloads all files from a list of urls."""
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        future_downloads = [executor.submit(download_single, url) for url in urls]
        for future in concurrent.futures.as_completed(future_downloads):
            print(future.result())


def parse_into_class(url_file: str) -> list[URL]:
    """Takes the raw data generated by the browser extension and parses it."""
    urls = []
    domain_regex = re.compile(r"//(.*?).kaltura")
    entry_id_regex = re.compile(r"entryId/(.*?)/")
    # real file to work on, need to test stuff first
    with open("urls_to_clean.txt", "r") as f:
        for line in f.readlines():
            url = line.split()[0]
            entry_id = entry_id_regex.search(url).group(1)
            domain = domain_regex.search(url).group(1)
            urls.append(URL(url, entry_id, domain))
    return urls


def trim_duplicates(urls: list[URL]) -> list[URL]:
    """Takes a list of urls and chooses one url for each entry_id."""
    final_urls = list(filter(lambda x: x.domain == "cdnapisec", urls))
    maybe_needed = list(filter(lambda x: x.domain != "cdnapisec", urls))

    for url in maybe_needed:
        if url.entry_id not in [x.entry_id for x in final_urls]:
            final_urls.append(url)
    return final_urls
    

def main():
    """Driver function."""
    urls = parse_into_class("urls_to_clean.txt")
    urls = trim_duplicates(urls)
    download_path = "G:\lecture-backup"
    download_all(urls, download_path)


if __name__ == "__main__":
    main()
